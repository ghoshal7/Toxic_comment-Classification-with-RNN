{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the data: 159571\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('train.csv')\n",
    "print(\"size of the data:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new features\n",
    "X1 = pd.DataFrame(columns=[])\n",
    "Y = pd.DataFrame(columns = ['toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "\n",
    "# Create Features - X1\n",
    "X1['length'] = data['comment_text'].apply(len)\n",
    "X1['num_words'] = data['comment_text'].str.split().apply(len)\n",
    "X1['caps_letter'] = data['comment_text'].apply(lambda capsL: sum(1 for c in capsL if c.isupper()))\n",
    "X1['caps_word'] = data['comment_text'].apply(lambda capsW : sum(1 for c in capsW.split() if c.isupper()))\n",
    "X1['exclaimations'] = data['comment_text'].apply(lambda exclaim : exclaim.count('!'))\n",
    "X1['questions'] = data['comment_text'].apply(lambda questions : questions.count('?'))\n",
    "X1['punct'] = data['comment_text'].apply(lambda punct: sum(punct.count(p) for p in '.,;:'))\n",
    "X1['sp_char'] = data['comment_text'].apply(lambda sp_char: sum(sp_char.count(s) for s in '@#$%^*()-'))\n",
    "X1['unique_words'] = data['comment_text'].apply(lambda text: len(set(w for w in text.split())))\n",
    "X1['num_emoji'] = data['comment_text'].apply(lambda emoji: sum(emoji.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "# Manipulate to find ratios\n",
    "X1['capsL_to_length'] = X1['caps_letter'] / X1['length']\n",
    "X1['capsW_to_length'] = X1['caps_word'] / X1['length']\n",
    "X1['unique_to_words'] = X1['unique_words']/X1['num_words']\n",
    "\n",
    "\n",
    "\n",
    "# Target values\n",
    "Y['toxic'] = data ['toxic']\n",
    "Y['severe_toxic'] = data['severe_toxic']\n",
    "Y['obscene'] = data['obscene']\n",
    "Y['threat'] = data['threat']\n",
    "Y['insult'] = data['insult']\n",
    "Y['identity_hate'] = data['identity_hate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of all_comments=  (159571,)\n"
     ]
    }
   ],
   "source": [
    "temp = data['comment_text']\n",
    "all_comments = []\n",
    "for i in range(len(temp)):\n",
    "    all_comments.append(temp[i])\n",
    "print('shape of all_comments= ', np.shape(all_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210337 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 500  # cut reviews after 500 words\n",
    "max_words = 1000000  # take top 1000,000 words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(all_comments)\n",
    "sequences = tokenizer.texts_to_sequences(all_comments)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data_token = pad_sequences(sequences, maxlen=maxlen)\n",
    "data_token1 = pd.DataFrame(data_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char = keras.preprocessing.text.Tokenizer(num_words=500,char_level=True)\n",
    "char.fit_on_texts(all_comments)\n",
    "char_seq = char.texts_to_sequences(all_comments)\n",
    "char_token = pad_sequences(char_seq, maxlen =500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 13)\n",
      "(159571, 500)\n",
      "(159571, 500)\n"
     ]
    }
   ],
   "source": [
    "X2 = char_token\n",
    "X3 = data_token\n",
    "print(np.shape(X1))\n",
    "print(np.shape(X2))\n",
    "print(np.shape(X3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghoshal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# features1 = np.hstack([X1, X3, X2])\n",
    "\n",
    "featrs = np.hstack([(0.1*X1), (0.6*X2), (0.3*X3)])\n",
    "labels = Y\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Train_text3, Test_text3, Train_label3, Test_label3 = train_test_split(featrs, labels, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/10\n",
      "151592/151592 [==============================] - 1426s 9ms/step - loss: 0.1170 - acc: 0.9660 - val_loss: 0.0772 - val_acc: 0.9750\n",
      "Epoch 2/10\n",
      "151592/151592 [==============================] - 1395s 9ms/step - loss: 0.0710 - acc: 0.9756 - val_loss: 0.0714 - val_acc: 0.9754\n",
      "Epoch 3/10\n",
      "151592/151592 [==============================] - 1397s 9ms/step - loss: 0.0622 - acc: 0.9777 - val_loss: 0.0735 - val_acc: 0.9745\n",
      "Epoch 4/10\n",
      "151592/151592 [==============================] - 1391s 9ms/step - loss: 0.0574 - acc: 0.9792 - val_loss: 0.0724 - val_acc: 0.9756\n",
      "Epoch 5/10\n",
      "151592/151592 [==============================] - 1398s 9ms/step - loss: 0.0622 - acc: 0.9774 - val_loss: 0.0723 - val_acc: 0.9758\n",
      "Epoch 6/10\n",
      "151592/151592 [==============================] - 1391s 9ms/step - loss: 0.0517 - acc: 0.9810 - val_loss: 0.0738 - val_acc: 0.9758\n",
      "Epoch 7/10\n",
      "151592/151592 [==============================] - 1390s 9ms/step - loss: 0.0480 - acc: 0.9823 - val_loss: 0.0745 - val_acc: 0.9744\n",
      "Epoch 8/10\n",
      "151592/151592 [==============================] - 1390s 9ms/step - loss: 0.0453 - acc: 0.9833 - val_loss: 0.0779 - val_acc: 0.9731\n",
      "Epoch 9/10\n",
      "151592/151592 [==============================] - 1390s 9ms/step - loss: 0.0426 - acc: 0.9842 - val_loss: 0.0808 - val_acc: 0.9722\n",
      "Epoch 10/10\n",
      "151592/151592 [==============================] - 1391s 9ms/step - loss: 0.0399 - acc: 0.9851 - val_loss: 0.0818 - val_acc: 0.9727\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Activation\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words, 32))\n",
    "model3.add(LSTM(32))\n",
    "model3.add(Dense(6, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "fit_lstm3 = model3.fit(Train_text3, Train_label3,epochs=10,batch_size=128, validation_data= (Test_text3, Test_label3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create X1\n",
    "X1_test = pd.DataFrame(columns=[])\n",
    "\n",
    "# Create Features - X1\n",
    "X1_test['length'] = test_data['comment_text'].apply(len)\n",
    "X1_test['num_words'] = test_data['comment_text'].str.split().apply(len)\n",
    "X1_test['caps_letter'] = test_data['comment_text'].apply(lambda capsL: sum(1 for c in capsL if c.isupper()))\n",
    "X1_test['caps_word'] = test_data['comment_text'].apply(lambda capsW : sum(1 for c in capsW.split() if c.isupper()))\n",
    "X1_test['exclaimations'] = test_data['comment_text'].apply(lambda exclaim : exclaim.count('!'))\n",
    "X1_test['questions'] = test_data['comment_text'].apply(lambda questions : questions.count('?'))\n",
    "X1_test['punct'] = test_data['comment_text'].apply(lambda punct: sum(punct.count(p) for p in '.,;:'))\n",
    "X1_test['sp_char'] = test_data['comment_text'].apply(lambda sp_char: sum(sp_char.count(s) for s in '@#$%^*()-'))\n",
    "X1_test['unique_words'] = test_data['comment_text'].apply(lambda text: len(set(w for w in text.split())))\n",
    "X1_test['num_emoji'] = test_data['comment_text'].apply(lambda emoji: sum(emoji.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "# Manipulate to find ratios\n",
    "X1_test['capsL_to_length'] = X1_test['caps_letter'] / X1['length']\n",
    "X1_test['capsW_to_length'] = X1_test['caps_word'] / X1['length']\n",
    "X1_test['unique_to_words'] = X1_test['unique_words']/X1['num_words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of all_comments=  (153164,)\n"
     ]
    }
   ],
   "source": [
    "temp_test = test_data['comment_text']\n",
    "test_comments = []\n",
    "for i in range(len(temp_test)):\n",
    "    test_comments.append(temp_test[i])\n",
    "print('shape of all_comments= ', np.shape(test_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 273192 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# X2\n",
    "\n",
    "maxlen = 500  # cut reviews after 500 words\n",
    "max_words = 1000000  # take top 1000,000 words\n",
    "\n",
    "tokenizer_test = Tokenizer(num_words=max_words)\n",
    "tokenizer_test.fit_on_texts(test_comments)\n",
    "sequences_test = tokenizer_test.texts_to_sequences(test_comments)\n",
    "word_index_test = tokenizer_test.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index_test))\n",
    "\n",
    "test_token = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "test_token1 = pd.DataFrame(data_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_test = keras.preprocessing.text.Tokenizer(num_words=500,char_level=True)\n",
    "char_test.fit_on_texts(test_comments)\n",
    "char_seq_test = char_test.texts_to_sequences(test_comments)\n",
    "char_token_test = pad_sequences(char_seq_test, maxlen =500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 13) (153164, 500) (153164, 500)\n"
     ]
    }
   ],
   "source": [
    "X2_test = test_token\n",
    "X3_test = char_token_test\n",
    "print(np.shape(X1_test), np.shape(X2_test), np.shape(X3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = np.hstack([(0.1*X1_test), (0.6*X2_test), (0.3*X3_test)])\n",
    "predict = model3.predict(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghoshal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "index = test_data[test_data.columns[0]]\n",
    "index = np.reshape(index, (len(test_data),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = np.hstack([index, predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "answer = pd.DataFrame(answer, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.020571</td>\n",
       "      <td>0.000136871</td>\n",
       "      <td>0.000994945</td>\n",
       "      <td>0.000561548</td>\n",
       "      <td>0.00140842</td>\n",
       "      <td>0.000714796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.0416148</td>\n",
       "      <td>0.000145815</td>\n",
       "      <td>0.00310565</td>\n",
       "      <td>0.000449397</td>\n",
       "      <td>0.00289282</td>\n",
       "      <td>0.000561309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.00872038</td>\n",
       "      <td>8.26892e-05</td>\n",
       "      <td>0.00085414</td>\n",
       "      <td>0.000269422</td>\n",
       "      <td>0.00079267</td>\n",
       "      <td>0.000255726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.0360134</td>\n",
       "      <td>0.000160344</td>\n",
       "      <td>0.00267686</td>\n",
       "      <td>0.000523436</td>\n",
       "      <td>0.0027998</td>\n",
       "      <td>0.000674375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.0166309</td>\n",
       "      <td>9.96004e-05</td>\n",
       "      <td>0.00116931</td>\n",
       "      <td>0.000312351</td>\n",
       "      <td>0.00128385</td>\n",
       "      <td>0.000350295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       toxic severe_toxic      obscene       threat  \\\n",
       "0  00001cee341fdb12    0.020571  0.000136871  0.000994945  0.000561548   \n",
       "1  0000247867823ef7   0.0416148  0.000145815   0.00310565  0.000449397   \n",
       "2  00013b17ad220c46  0.00872038  8.26892e-05   0.00085414  0.000269422   \n",
       "3  00017563c3f7919a   0.0360134  0.000160344   0.00267686  0.000523436   \n",
       "4  00017695ad8997eb   0.0166309  9.96004e-05   0.00116931  0.000312351   \n",
       "\n",
       "       insult identity_hate  \n",
       "0  0.00140842   0.000714796  \n",
       "1  0.00289282   0.000561309  \n",
       "2  0.00079267   0.000255726  \n",
       "3   0.0027998   0.000674375  \n",
       "4  0.00128385   0.000350295  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.to_csv('Submission_Gourav.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
